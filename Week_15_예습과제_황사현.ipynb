{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **8.6 토픽 모델링(Topic Modeling)-20 뉴스그룹**"
      ],
      "metadata": {
        "id": "mVeqaUiZ0m0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토픽 모델링(Topic Modeling)**: 문서 집합에 숨어 있는 주제를 찾아내는 것\n",
        "- 머신러닝 기반의 토픽 모델링을 적용해 숨어있는 중요 주제를 효과적으로 찾아내는 것이 가능.\n",
        "- 머신러닝 기반의 토픽 모델은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출.\n",
        "\n",
        "    - ⭐️ **LSA(Latent Semantic Analysis)**\n",
        "    - **LDA(Latent Dirichlet Allocation)**"
      ],
      "metadata": {
        "id": "CULtWj_r03Tw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCobm7pmzwGh",
        "outputId": "86da4e33-913b-4559-b72a-1c8e2b27db75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Shape: (7862, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x', 'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "\n",
        "news_df= fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)\n",
        "\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
        "feat_vect = count_vect.fit_transform(news_df.data)\n",
        "print('CountVectorizer Shape:', feat_vect.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
        "lda.fit(feat_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Ijd8CAzs2iMX",
        "outputId": "769e5b4e-77f2-4ea1-be69-ea6436c28ecf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=8, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 속성값 확인\n",
        "print(lda.components_.shape)\n",
        "lda.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD19c74G2qOl",
        "outputId": "3a132b59-a0af-41aa-9fb9-138dde7d9442"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
              "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
              "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
              "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
              "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
              "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
              "       ...,\n",
              "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
              "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
              "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
              "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
              "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
              "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_index, topic in enumerate(model.components_):\n",
        "        print('Topic #', topic_index)\n",
        "\n",
        "        topic_word_indexes = topic.argsort()[::-1]\n",
        "        top_indexes=topic_word_indexes[:no_top_words]\n",
        "\n",
        "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
        "        print(feature_concat)\n",
        "\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "\n",
        "display_topics(lda, feature_names, 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfizQu4s2xcQ",
        "outputId": "954262ae-895e-48b6-95a4-63ec9796b8e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic # 0\n",
            "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
            "Topic # 1\n",
            "don just like know people said think time ve didn right going say ll way\n",
            "Topic # 2\n",
            "image file jpeg program gif images output format files color entry 00 use bit 03\n",
            "Topic # 3\n",
            "like know don think use does just good time book read information people used post\n",
            "Topic # 4\n",
            "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
            "Topic # 5\n",
            "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
            "Topic # 6\n",
            "god people jesus church believe christ does christian say think christians bible faith sin life\n",
            "Topic # 7\n",
            "use dos thanks windows using window does display help like problem server need know run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O2ocVQBB385S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ya-ycK73TnQ",
        "outputId": "9e870f11-4e12-4571-babe-30b58ca65b6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
            "\n",
            "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
            " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |  \n",
            " |  Convert a collection of text documents to a matrix of token counts.\n",
            " |  \n",
            " |  This implementation produces a sparse representation of the counts using\n",
            " |  scipy.sparse.csr_matrix.\n",
            " |  \n",
            " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            " |  that does some kind of feature selection then the number of features will\n",
            " |  be equal to the vocabulary size found by analyzing the data.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  input : {'filename', 'file', 'content'}, default='content'\n",
            " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
            " |        expected to be a list of filenames that need reading to fetch\n",
            " |        the raw content to analyze.\n",
            " |  \n",
            " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            " |        object) that is called to fetch the bytes in memory.\n",
            " |  \n",
            " |      - If `'content'`, the input is expected to be a sequence of items that\n",
            " |        can be of type string or byte.\n",
            " |  \n",
            " |  encoding : str, default='utf-8'\n",
            " |      If bytes or files are given to analyze, this encoding is used to\n",
            " |      decode.\n",
            " |  \n",
            " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            " |      Instruction on what to do if a byte sequence is given to analyze that\n",
            " |      contains characters not of the given `encoding`. By default, it is\n",
            " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            " |      values are 'ignore' and 'replace'.\n",
            " |  \n",
            " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
            " |      Remove accents and perform other character normalization\n",
            " |      during the preprocessing step.\n",
            " |      'ascii' is a fast method that only works on characters that have\n",
            " |      a direct ASCII mapping.\n",
            " |      'unicode' is a slightly slower method that works on any characters.\n",
            " |      None (default) does nothing.\n",
            " |  \n",
            " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
            " |      :func:`unicodedata.normalize`.\n",
            " |  \n",
            " |  lowercase : bool, default=True\n",
            " |      Convert all characters to lowercase before tokenizing.\n",
            " |  \n",
            " |  preprocessor : callable, default=None\n",
            " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
            " |      preserving the tokenizing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  tokenizer : callable, default=None\n",
            " |      Override the string tokenization step while preserving the\n",
            " |      preprocessing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |  stop_words : {'english'}, list, default=None\n",
            " |      If 'english', a built-in stop word list for English is used.\n",
            " |      There are several known issues with 'english' and you should\n",
            " |      consider an alternative (see :ref:`stop_words`).\n",
            " |  \n",
            " |      If a list, that list is assumed to contain stop words, all of which\n",
            " |      will be removed from the resulting tokens.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |      If None, no stop words will be used. In this case, setting `max_df`\n",
            " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
            " |      and filter stop words based on intra corpus document frequency of terms.\n",
            " |  \n",
            " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            " |      Regular expression denoting what constitutes a \"token\", only used\n",
            " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            " |      or more alphanumeric characters (punctuation is completely ignored\n",
            " |      and always treated as a token separator).\n",
            " |  \n",
            " |      If there is a capturing group in token_pattern then the\n",
            " |      captured group content, not the entire match, becomes the token.\n",
            " |      At most one capturing group is permitted.\n",
            " |  \n",
            " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            " |      The lower and upper boundary of the range of n-values for different\n",
            " |      word n-grams or char n-grams to be extracted. All values of n such\n",
            " |      such that min_n <= n <= max_n will be used. For example an\n",
            " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            " |      Whether the feature should be made of word n-gram or character\n",
            " |      n-grams.\n",
            " |      Option 'char_wb' creates character n-grams only from text inside\n",
            " |      word boundaries; n-grams at the edges of words are padded with space.\n",
            " |  \n",
            " |      If a callable is passed it is used to extract the sequence of features\n",
            " |      out of the raw, unprocessed input.\n",
            " |  \n",
            " |      .. versionchanged:: 0.21\n",
            " |  \n",
            " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            " |      first read from the file and then passed to the given callable\n",
            " |      analyzer.\n",
            " |  \n",
            " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly higher than the given threshold (corpus-specific\n",
            " |      stop words).\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly lower than the given threshold. This value is also\n",
            " |      called cut-off in the literature.\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  max_features : int, default=None\n",
            " |      If not None, build a vocabulary that only consider the top\n",
            " |      `max_features` ordered by term frequency across the corpus.\n",
            " |      Otherwise, all features are used.\n",
            " |  \n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  vocabulary : Mapping or iterable, default=None\n",
            " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            " |      indices in the feature matrix, or an iterable over terms. If not\n",
            " |      given, a vocabulary is determined from the input documents. Indices\n",
            " |      in the mapping should not be repeated and should not have any gap\n",
            " |      between 0 and the largest index.\n",
            " |  \n",
            " |  binary : bool, default=False\n",
            " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
            " |      probabilistic models that model binary events rather than integer\n",
            " |      counts.\n",
            " |  \n",
            " |  dtype : dtype, default=np.int64\n",
            " |      Type of the matrix returned by fit_transform() or transform().\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  vocabulary_ : dict\n",
            " |      A mapping of terms to feature indices.\n",
            " |  \n",
            " |  fixed_vocabulary_ : bool\n",
            " |      True if a fixed vocabulary of term to indices mapping\n",
            " |      is provided by the user.\n",
            " |  \n",
            " |  stop_words_ : set\n",
            " |      Terms that were ignored because they either:\n",
            " |  \n",
            " |        - occurred in too many documents (`max_df`)\n",
            " |        - occurred in too few documents (`min_df`)\n",
            " |        - were cut off by feature selection (`max_features`).\n",
            " |  \n",
            " |      This is only available if no vocabulary was given.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  HashingVectorizer : Convert a collection of text documents to a\n",
            " |      matrix of token counts.\n",
            " |  \n",
            " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            " |      of TF-IDF features.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The ``stop_words_`` attribute can get large and increase the model size\n",
            " |  when pickling. This attribute is provided only for introspection and can\n",
            " |  be safely removed using delattr or set to None before pickling.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            " |  >>> corpus = [\n",
            " |  ...     'This is the first document.',\n",
            " |  ...     'This document is the second document.',\n",
            " |  ...     'And this is the third one.',\n",
            " |  ...     'Is this the first document?',\n",
            " |  ... ]\n",
            " |  >>> vectorizer = CountVectorizer()\n",
            " |  >>> X = vectorizer.fit_transform(corpus)\n",
            " |  >>> vectorizer.get_feature_names_out()\n",
            " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            " |         'this'], ...)\n",
            " |  >>> print(X.toarray())\n",
            " |  [[0 1 1 1 0 0 1 0 1]\n",
            " |   [0 2 0 1 0 1 1 0 1]\n",
            " |   [1 0 0 1 1 0 1 1 1]\n",
            " |   [0 1 1 1 0 0 1 0 1]]\n",
            " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
            " |  >>> vectorizer2.get_feature_names_out()\n",
            " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
            " |         'this document', 'this is', 'this the'], ...)\n",
            " |   >>> print(X2.toarray())\n",
            " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CountVectorizer\n",
            " |      _VectorizerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, raw_documents, y=None)\n",
            " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted vectorizer.\n",
            " |  \n",
            " |  fit_transform(self, raw_documents, y=None)\n",
            " |      Learn the vocabulary dictionary and return document-term matrix.\n",
            " |      \n",
            " |      This is equivalent to fit followed by transform, but more efficiently\n",
            " |      implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : array of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Transformed feature names.\n",
            " |  \n",
            " |  inverse_transform(self, X)\n",
            " |      Return terms per document with nonzero entries in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_inv : list of arrays of shape (n_samples,)\n",
            " |          List of arrays of terms.\n",
            " |  \n",
            " |  transform(self, raw_documents)\n",
            " |      Transform documents to document-term matrix.\n",
            " |      \n",
            " |      Extract token counts out of raw text documents using the vocabulary\n",
            " |      fitted with fit or the one provided to the constructor.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : sparse matrix of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  build_analyzer(self)\n",
            " |      Return a callable to process input data.\n",
            " |      \n",
            " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      analyzer: callable\n",
            " |          A function to handle preprocessing, tokenization\n",
            " |          and n-grams generation.\n",
            " |  \n",
            " |  build_preprocessor(self)\n",
            " |      Return a function to preprocess the text before tokenization.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      preprocessor: callable\n",
            " |            A function to preprocess the text before tokenization.\n",
            " |  \n",
            " |  build_tokenizer(self)\n",
            " |      Return a function that splits a string into a sequence of tokens.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tokenizer: callable\n",
            " |            A function to split a string into a sequence of tokens.\n",
            " |  \n",
            " |  decode(self, doc)\n",
            " |      Decode the input into a string of unicode symbols.\n",
            " |      \n",
            " |      The decoding strategy depends on the vectorizer parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc : bytes or str\n",
            " |          The string to decode.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      doc: str\n",
            " |          A string of unicode symbols.\n",
            " |  \n",
            " |  get_stop_words(self)\n",
            " |      Build or fetch the effective stop words list.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      stop_words: list or None\n",
            " |              A list of stop words.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8.7 문서 군집화 소개와 실습(Opinion Review 데이터 세트)**"
      ],
      "metadata": {
        "id": "J2m0uVwP3-9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**문서 군집화(Document Clustering)**:  비슷한 텍스트 구성의 문서를 군집화(Clustering)하는 것\n",
        "\n",
        "- 학습 데이터 세트가 필요없는 비지도학습 기반으로 동작.\n",
        "\n",
        "**Opinion Review 데이터 세트를 이요한 문서 군집화 수행하기**"
      ],
      "metadata": {
        "id": "UN3cnPZ64G6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob, os\n",
        "\n",
        "path = '/Users/hwangsahyun/topics/'\n",
        "\n",
        "all_files = glob.glob(os.path.join(path, '*.data'))\n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in all_files:\n",
        "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
        "\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "\n",
        "    filename_list.append(filename)\n",
        "    opinion_text.append(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "document_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ITieHFu23gaq",
        "outputId": "1c2f490c-b492-4ac5-ae2c-a12fecb131d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [filename, opinion_text]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-243d2615-4a6b-4d81-9317-afa5dbc90a17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>opinion_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-243d2615-4a6b-4d81-9317-afa5dbc90a17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-243d2615-4a6b-4d81-9317-afa5dbc90a17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-243d2615-4a6b-4d81-9317-afa5dbc90a17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "document_df",
              "summary": "{\n  \"name\": \"document_df\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"opinion_text\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "NshfaliP7pgV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rom sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
        "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
        "\n",
        "#opinion_text 컬럼값으로 feature vectorization 수행\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
      ],
      "metadata": {
        "id": "ARv5hjCW8Shh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfvect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', ngram_range=(1, 2), min_df=0.05, max_df=0.85)\n",
        "\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
      ],
      "metadata": {
        "id": "qjCQ5MdW66ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster miport KMeans\n",
        "\n",
        "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_states=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_"
      ],
      "metadata": {
        "id": "U1c_aN-98VcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
      ],
      "metadata": {
        "id": "AQnG1uir8ghY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "\n",
        "document_df['cluster_label'] = cluster_label\n",
        "document_df.sort_values(by='cluster_label')"
      ],
      "metadata": {
        "id": "I5hCja9Y8ppk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **군집별 핵심 단어 추출하기**"
      ],
      "metadata": {
        "id": "cutLCZPC80mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_centers = km_cluster.cluster_centers_\n",
        "print('cluster_centers_shape :', cluster_centers.shape)\n",
        "print(cluster_centers)"
      ],
      "metadata": {
        "id": "cee9QFZj8zzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
        "    cluster_details = {}\n",
        "\n",
        "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "    for cluster_num in range(clusters_num):\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "\n",
        "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
        "        top_features = [feature_names[ind] for ind in top_feature_indexes]\n",
        "\n",
        "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
        "\n",
        "        cluster_details[cluster_num]['top_features'] = top_features\n",
        "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
        "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
        "        filenames = filenames.values.tolist()\n",
        "\n",
        "        cluster_details[cluster_num]['filenames'] = filenames\n",
        "\n",
        "    return cluster_details"
      ],
      "metadata": {
        "id": "e1b6SjvN87-B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print('####### Cluster {0}'.format(cluster_num))\n",
        "        print('Top features:', cluster_detail['top_features'])\n",
        "        print('Reviews 파일명:', cluster_detail['filenames'][:7])\n",
        "        print('==============================================')"
      ],
      "metadata": {
        "id": "68qYh8fF9qo-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tfidf_vect.get_feature_names_out()\n",
        "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df, feature_names=feature_names, clusters_num=3, top_n_features=10 )\n",
        "print_cluster_details(cluster_details)"
      ],
      "metadata": {
        "id": "OgqdCDjF-TYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8.8 문서 유사도**"
      ],
      "metadata": {
        "id": "YHwLLAej-Y0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**문서 유사도 측정 방법 - 코사인 유사도**\n",
        "\n",
        ": 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호 방향성이 얼마나 유사한지에 기반.\n",
        "\n",
        ": 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것."
      ],
      "metadata": {
        "id": "uB8prz8K-eRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def cos_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    l2_norm =(np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
        "    similarity = dot_product / l2_norm\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "LZHQ7KwO-X0Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc_list =['if you take the blue pill, the story ends',\n",
        "           'if you take the red pill, you stay in Wonderland',\n",
        "           'if you take the red pill, I show you how deep the rabbit hole goes']\n",
        "\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYTzrnAP-7UK",
        "outputId": "46bdf0ae-9c91-4f85-9329-9220b9094b23"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
        "\n",
        "similarity_simple = cos_similarity(vect1, vect2)\n",
        "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZxdr21J_Rr2",
        "outputId": "61be55a5-60fc-4dad-c8c9-8842527855db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 2 Cosine 유사도: 0.402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
        "vect3 = np.array(feature_vect_dense[2]) .reshape(-1, )\n",
        "similarity_simple = cos_similarity(vect1, vect3)\n",
        "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
        "\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
        "similarity_simple = cos_similarity(vect2, vect3)\n",
        "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OHnHT6-_tLR",
        "outputId": "c9093cd4-b64e-4c16-8e7f-7e3173d46adb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 3 Cosine 유사도: 0.404\n",
            "문장 2, 문장 3 Cosine 유사도: 0.456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple)\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KpN9I9bAB1e",
        "outputId": "41295554-fa1e-4059-f7b7-d3bc5ab2fbd5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCv9za8rAMVu",
        "outputId": "3c74db91-b601-43d9-c39c-ef778ee6dad1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_simple_pair = cosine_similarity(feature_vect_simple, feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print('shape:', similarity_simple_pair.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KwwWE9oAYLT",
        "outputId": "41d4ff91-e667-45cc-c5d8-31119b2328e6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]\n",
            " [0.40207758 1.         0.45647296]\n",
            " [0.40425045 0.45647296 1.        ]]\n",
            "shape: (3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8.9 한글 텍스트 처리 - 네이버 영화 평점 감성 분석**"
      ],
      "metadata": {
        "id": "qiq8mzwnAn5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**한글 NLP 처리의 어려움**\n",
        "\n",
        ": '띄어쓰기'와 '다양한 조사' 때문에 라틴어 처리보다 어려움.\n",
        "\n",
        "- 띄어쓰기를 잘못하면 의미가 왜곡되어 전달될 수 있기 때문\n",
        "- 조사는 주어나 목적어를 위해 추가되고, 경우의 수가 많기 때문에 전처리 시의 제거가 까다로움.\n",
        "\n",
        "\n",
        "**KoNLPy 소개**\n",
        "\n",
        ": 파이썬의 대표적인 한글 형태소 패키지"
      ],
      "metadata": {
        "id": "ie2azZHYAv5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/ratings_train.txt', sep='\\t')\n",
        "train_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "tO9VOP4ZAfCh",
        "outputId": "1bc97d2d-e898-4c6f-e8ad-970ea5712ef5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                           document  label\n",
              "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d78b557-88d2-4ec6-9372-c3daf6b33fc1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d78b557-88d2-4ec6-9372-c3daf6b33fc1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d78b557-88d2-4ec6-9372-c3daf6b33fc1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d78b557-88d2-4ec6-9372-c3daf6b33fc1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7acd5baf-c121-493c-bd09-2395a857d475\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7acd5baf-c121-493c-bd09-2395a857d475')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7acd5baf-c121-493c-bd09-2395a857d475 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pc8Ra02B0sx",
        "outputId": "6a07736c-a6bc-47b8-e9b5-21f1a454f08a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    75173\n",
              "1    74827\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "train_df = train_df.fillna(' ')\n",
        "train_df['document'] = train_df['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
        "\n",
        "test_df = pd.read_csv('/content/ratings_train.txt', sep='\\t')\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df['document'] = test_df['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
        "\n",
        "train_df.drop('id', axis=1, inplace=True)\n",
        "test_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "uP5mBgXiB8uX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbtLZbw_JZ2V",
        "outputId": "1bd72f57-c058-4cb2-8ec6-b6c5ddb2a19d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Twitter\n",
        "\n",
        "twitter = Twitter()\n",
        "def tw_tokenizer(text):\n",
        "    tokens_ko = twitter.morphs(text)\n",
        "    return tokens_ko"
      ],
      "metadata": {
        "id": "USK_ctgZCavi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df['document'])\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])\n",
        "\n",
        "lg_clf = LogisticRegression(random_state=0)\n",
        "params = {'C': [1, 3.5, 4.5, 5.5, 10]}\n",
        "grid_cv = GridSearchCV(lg_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv.fit(tfidf_matrix_train, train_df['label'])\n",
        "print(grid_cv.best_params_, round(grid_cv.best_score_, 4))"
      ],
      "metadata": {
        "id": "s5XnnAKLCqB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C가 3.5일 때 최고 0.8593의 정확도를 보임."
      ],
      "metadata": {
        "id": "NCqt8YeMKotN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
        "\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "preds = best_estimator.predict(tfidf_matrix_test)\n",
        "\n",
        "print('Logistic Regression 정확도: ' , accuracy_score(test_df['label'], preds))"
      ],
      "metadata": {
        "id": "uh7d8i5ZKvUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}